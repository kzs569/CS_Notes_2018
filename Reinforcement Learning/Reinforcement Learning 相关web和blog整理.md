Reinforcement Learning 相关web和blog整理

[Deliberation Networks:Sequence Generation Beyond One-Pass Decoding[NIPS2017]用于序列生成的推敲网络](<https://www.msra.cn/zh-cn/news/features/nips17-online-sharing-lijun-wu-20171206>)

机器翻译中的对偶学习 ： <https://www.msra.cn/zh-cn/news/features/dual-learning-20161207>

强化学习的泡沫：强化学习路在何方？<https://zhuanlan.zhihu.com/p/39999667>

OpenAI Five：<https://openai.com/blog/openai-five/>

How to Train Your OpenAI Five ： <https://openai.com/blog/how-to-train-your-openai-five/>

微软亚洲研究院2019年CVPR顶会交流 ：<https://ai.yanxishe.com/page/meeting/44>

Simple Q-learning for Poker demo : <https://www.jiqizhixin.com/articles/2017-10-15-6>

ICML论文｜阿尔法狗CTO讲座： AI如何用新型强化学习玩转围棋扑克游戏 (main on NFSP)<https://www.leiphone.com/news/201606/5ekkj6w511ZK5yOW.html>

德州扑克AI(Libratus)的背后：不完美信息博弈中，求解安全嵌套的子博弈, #NIPS 2017最佳论文奖 

[http://nooverfit.com/wp/%E5%BE%B7%E5%B7%9E%E6%89%91%E5%85%8Bailibratus%E7%9A%84%E8%83%8C%E5%90%8E%EF%BC%9A%E4%B8%8D%E5%AE%8C%E7%BE%8E%E4%BF%A1%E6%81%AF%E5%8D%9A%E5%BC%88%E4%B8%AD%EF%BC%8C%E6%B1%82%E8%A7%A3%E5%AE%89/](http://nooverfit.com/wp/德州扑克ailibratus的背后：不完美信息博弈中，求解安/)

Playing a toy poker game with Reinforcement Learning : <http://willtipton.com/coding/poker/2017/06/06/shove-fold-with-reinforcement-learning.html>

Solving the Shove/fold Game with TensorFlow:<http://willtipton.com/coding/poker/2016/03/06/shove-fold-with-tensorflow.html>

Reviews of *Deep reinforcement learning with relational inductive biases* :  <https://openreview.net/forum?id=HkxaFoC9KQ>

NAACL 相关论文及best paper BERT : <https://mp.weixin.qq.com/s/IZ5qDexQ4yDcKUdz_DZCTw>

Distributed Proximal Policy Optimization (DPPO) 莫烦PYTHON : <https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/6-4-DPPO/>

重拾基础 - A3C & DPPO <https://zhuanlan.zhihu.com/p/38771094>

最前沿：当我们以为Rainbow就是Atari游戏的巅峰时，Ape-X出来把Rainbow秒成了渣！<https://zhuanlan.zhihu.com/p/36375292>

DRL之Policy Gradient, Deterministic Policy Gradient与Actor Critic <https://zhuanlan.zhihu.com/p/26882898>

从Fictitious Play 到 NFSP [https://gyh75520.github.io/2017/07/27/%E4%BB%8EFictitious%20Play%20%E5%88%B0%20NFSP/](https://gyh75520.github.io/2017/07/27/从Fictitious Play 到 NFSP/)

论文：Mix & Match – Agent Curricula for Reinforcement Learning 笔记 <https://zhuanlan.zhihu.com/p/39242814>

强化学习从入门到放弃的资料 <https://github.com/wwxFromTju/awesome-reinforcement-learning-zh>

Eighteen Months of RL Research at Google Brain in Montreal <http://www.marcgbellemare.info/blog/eighteen-months-of-rl-research-at-google-brain-in-montreal/>

<https://www.leiphone.com/news/201903/6UJUua8oGZnc6Bj6.html>

漫谈基于模型的强化学习方法 PILCO - Probabilistic Inference for Learning Control <https://blog.csdn.net/philthinker/article/details/79749038>

OpenAI基线新实现ACKTR与A2C：把置信域优化应用到强化学习 <https://www.jiqizhixin.com/articles/2017-08-19-6>

Scaling Multi-Agent Reinforcement Learning <https://bair.berkeley.edu/blog/2018/12/12/rllib/>

Assessing Generalization in Deep Reinforcement Learning <https://bair.berkeley.edu/blog/2019/03/18/rl-generalization/>

RL — Proximal Policy Optimization (PPO) Explained <https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12>



Multi-Agent Environment : 

Neural MMO: A Massively Multiagent Game Environment <https://openai.com/blog/neural-mmo/>

Capture the Flag: the emergence of complex cooperative agents <https://deepmind.com/blog/capture-the-flag/>



Github : 

Pytorch Implementation of MADDPG : <https://github.com/xuehy/pytorch-maddpg>

OpenAI Implementation of MADDPG:<https://github.com/openai/maddpg>

Related Multi-Agent Particle Environment : <https://github.com/openai/multiagent-particle-envs>

Some Implementation of Recent RL papers : <https://github.com/carpedm20/deep-rl-tensorflow>

Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures : <https://github.com/deepmind/scalable_agent>

TensorSwarm: A framework for reinforcement learning of robot swarms. <https://github.com/TensorSwarm/TensorSwarm>

Multi-Agent PPO <https://github.com/jsztompka/MultiAgent-PPO>

