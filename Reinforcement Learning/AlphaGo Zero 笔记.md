# AlphaGo Zero 笔记

*key word : Deep Learning, Reinforcement Learning, MCTS*

 AlphaGo Zero 可以被理解成是一种近似策略迭代，并在其训练循环中集成了MCTS以改进策略和评估策略。MCTS 可以被看作是一种策略改进算子（policy improvement operator），可以输出比神经网络的原始概率更强的走子概率。使用搜索的自我对弈可以被看作是一种策略评估算子（policy evaluation operator）。它使用了 MCTS 来选择走子，并将对弈赢家看作是价值函数的样本。然后这个策略迭代过程会更新神经网络的权重，从而将走子的概率和价值与提升后的搜索概率和自我对弈赢家更紧密地匹配起来，然后再在下一次迭代中使用更新后的神经网络权重执行自我对弈，以使其搜索更加强大。

与 AlphaGo 相比，AlphaGo Zero 的特性有：

1. 它是从随机对弈学习的，使用了自我对弈强化学习，没有使用人类数据或监督；

2. 它使用了棋盘上的黑白子作为输入，而没有使用任何人类参与的特征工程；

3. 它使用了单个神经网络来同时表征策略和价值，而没使用单独的策略网络和价值网络；

4. 它的局面评估使用了神经网络，MCTS 使用了走子采样，而没有执行蒙特卡洛 rollout。

### 问题描述

围棋问题，棋盘19*19=361个交叉点可供落子，每个点三种状态，白（用`1`表示），黑（用`-1`表示），无子（用`0`表示），用$\vec{s}​$ **描述**此时**棋盘的状态**，即棋盘的**状态向量**记为 $\vec{s}​$ （state首字母）
$$
\vec{s}=(\underbrace{1,0,-1, \ldots}_{361})
$$
假设状态$\vec{s}​$下，暂不考虑不能落子的情况，那么下一步可走的位置空间也是361个。将下一步的落子行动也用一个361维的向量来表示，记为$\vec{a}​$
$$
\vec{a}=(0, \dots, 0,1,0, \dots)
$$

由以上定义，我们把围棋问题转化为

> 任意给定一个状态$\vec{s}​$，寻找最优的应对策略$\vec{a}​$,最终可以获取棋盘上的最大地盘

### 网络结构

新的网络中，使用了一个参数为$\theta$（需要通过训练来不断调整）的深度神经网络$f_\theta$

- **网络输入** ： $19 \times 19 \times 17$ 0/1值：现在棋盘状态$\vec{s}$以及7步历史落子记录。最后一个位置记录黑白，0白1黑

- **网络输出** ：落子概率（362个输出值）和一个评估值（[-1,1]之间）记为$f_\theta(\vec{s})=(p,v)$

  - 落子概率p：p向量表示下一步在每一个可能位置落子的概率，又称先验概率（加上不下的选择），即$p_a=Pr(\vec{a}|\vec{s})$(在当前输入条件下在每个可能点落子的概率)

  - 评估值v：表示现在准备下当前这步棋的选手在**输入的这八步历史局面**$\vec{s}$**下的胜率**

- **网络结构**：基于ResNet, 包含20或40个Residual Block, 加入Batch Normalization 和 非线性整流器rectifier non-linear模块

  

### 改进的强化学习算法

**自对弈强化学习算法**

在每一个状态$\vec{s}$,利用深度神经网络$f_\theta$预测作为参照执行MCTS搜索，MCTS搜索的输出是每一个状态下在不同位置对应的概率$\pi$(这里$\pi$是一个361维的向量，里面的值是MCTS搜索得出的概率值)，一种策略，从人类的眼光来看，就是看到现在局面，选择在每个不同的落子的点的概率。如下面的公式的例子，下在（1，3）位置的概率是0.92，有很高概率选这个点作为落子点
$$
\boldsymbol{\pi}_{i}=(\underbrace{0.01,0.02,0.92, \ldots}_{361})
$$
MCTS搜索得出的落子概率比$f_\theta$输出的仅使用神经网络输出的落子概率p更强，因此，MCTS可以被视为一个**强力的策略改善（policy improvement）过程**

使用基于MCTS提升后的策略（policy）来进行落子，然后用自对弈最终对局的胜者 z 作为价值（Value），作为一个强力的**策略评估（policy evaluation）过程**

并用上述的规则，完成一个**通用策略迭代算法**去更新神经网络的**参数** $\theta$,使得神经网络输出的落子概率和评估值，即 $f_\theta(\vec{s})=(p,v)$ 更加贴近**能把这盘棋局赢下的落子方式**（使用不断提升的MCTS搜索落子策略$\pi$ 和自对弈的胜者 z 作为调整依据）。并且，在下轮迭代中使用新的参数来进行自对弈

在这里补充强化学习的通用策略（Generalized Policy Iteration）方法：
- 从策略$\pi_0$开始
- 策略评估（Policy Evaluation）- 得到策略$\pi_0$的价值$v_{\pi_0}$
- 策略改善（Policy Improvement）- 根据价值$v_{\pi_0}$,优化策略为 $\pi_{0+1}$（即人类学习的过程，加强对棋局的判断能力，做出更好的判断）
- 迭代上面的步骤2和3，直到找到最优价值$v_*$,可以得到最优策略$\pi_*$

![](http://ww1.sinaimg.cn/large/006ocvumgy1g1vczg88cyj31la1fitow.jpg)

a图 表示自对弈过程$s_1,...,s_T​$。在每个位置$s_t​$,使用最新的神经网络$f_\theta​$执行一次MCTS搜索$\alpha_\theta​$。根据搜索得出的概率$a_t \sim\pi_i​$进行落子。终局$s_T​$时根据围棋规则计算胜者z

$\pi_i​$是每一步执行MCTS搜索时得出的结果（柱状图表示概率的高低）

b图 表示更新神经网络参数过程。使用原始落子状态$\vec{s_t}​$作为输入，得到此棋盘状态$\vec{s_t}​$下下一步所有可能落子位置的概率分布$p_t​$和当前状态$\vec{s_t}​$下选手的赢棋评估值$v_t​$

以最大化$p_t$与$\pi_t$相似度和最小化预测的胜者$v_t$和局终胜者z的误差来更新神经网络参数$\theta$，更新参数$\theta$，下一轮迭代中使用新神经网络进行自我对弈



我们知道，最初的MCTS搜索算法是使用随机来进行模拟，在AlphaGo 1.0中使用局面函数辅助策略函数作为落子的参考进行模拟。在最新的模型中，MCT使用神经网络$f_\theta$的输出来作为落子的参考

每一条边$(\vec{s},\vec{a})$(每个状态下的落子选择)保存的是三个值：先验概率$P(\vec{s},\vec{a})$，访问次数$N(\vec{s},\vec{a})$，行动价值$Q(\vec{s},\vec{a})$。

每次模拟（模拟一盘棋，直到分出胜负）从根状态开始，每次落子最大化上限置信区间$Q(\vec{s},\vec{a})+U(\vec{s},\vec{a})$ 其中$U(\vec{s}, \vec{a}) \propto \frac{P(\vec{s}, \vec{a})}{1+N(\vec{s}, \vec{a})}$直到遇到叶子节点$s^{\prime}​$

叶子节点（终局）只会被产生一次用于产生先验概率和评估值，符号表示即$f_{\theta}\left(s^{\prime}\right)=\left(P\left(s^{\prime}, \cdot\right), V\left(s^{\prime}\right)\right)$

模拟过程中遍历每条边$(\vec{s},\vec{a})$时更新记录的统计数据。访问次数加一$N(\vec{s},\vec{a})+=1$；更新行动价值为整个模拟过程的平均值，即$Q(\vec{s}, \vec{a})=\frac{1}{N(\vec{s}, \vec{a})} \Sigma_{\vec{s}^{\prime} | \vec{s}, \vec{a} \Rightarrow \vec{s}} V\left(\vec{s}^{\prime}\right)$，$\vec{s}^{\prime} | \vec{s}, \vec{a} \Rightarrow \vec{s}^{\prime}$表示在模拟过程中从$\vec{s}$走到${\vec{s}}^{\prime}$的所有落子行动$\vec{a}​$

[深入浅出看懂AlphaGo元](https://charlesliuyx.github.io/2017/10/18/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%9C%8B%E6%87%82AlphaGo%E5%85%83/)
[深入浅出看懂AlphaGo如何下棋](https://charlesliuyx.github.io/2017/05/27/AlphaGo%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/)
​    

``` python


Input:棋盘局面、历史和要下的颜色的原始棋盘表示，以19*19的图像形式提供：游戏规则；一个对弈评价函数；在旋转和翻转时的游戏规则不变性；颜色转换情况下的不变性（除了贴目的情况）

Output:策略p, 价值v

随机地初始化神经网络权重$\theta_0$

//AlphaGo Zero 遵循一种策略迭代过程

for 每次迭代i do

​	//终止条件：

​	//1. 两个棋手都放弃走子

​	//2.搜索值降到投子认负的阈值以下

​	//3.游戏超过最大步数

​	初始化$s_0$

​	for 每个步骤t，直到终止于步骤T do

​		//MCST可以被看作是一种策略改进算子

​		//搜索算法：异步策略和价值MCST算法（APV-MCST）

​		//使用之前的神经网络$f_{\theta_{i=1}}$执行MCST搜索$\pi_t=\alpha_{\theta{i=1} }(s_t)$

​		//该搜索树中的每一个边(s,a)都存储了一个先验概率P(s,a)，一个访问计数N(s,a), 一个动作值Q(s,a)

​		while 还有剩余计算资源 do

​			选择：每次模拟都通过选择带有最大置信区间上界Q(s,a)+U(s,a)的边来遍历这个树，其中$U(s, a) \propto P(s, a) /(1+N(s, a))​$

​			扩展和评估：使用神经网络扩展叶节点并评估关联的局面s，$(P(s, \cdot), V(s))=f_{\theta_{i}}(s)​$ P值的向量保存在从s出来的边上。

​			备份：在该模拟中遍历的每个边(s,a)都被上传以增加其访问计数N(s,a)，并将其动作值更新为在这些模拟上的平均估计$Q(s, a)=1 / N(s, a) \sum_{s^{\prime}|s, a \rightarrow s^{\prime}} V\left(s^{\prime}\right)$其中$s^{\prime}|s, a \rightarrow s^{\prime}$表示该模拟在采取了走法a之后从局面s变成了局面$s^{\prime}$

​		end

​		//使用搜索的自我对弈可以被看作是一种策略评估算子：使用改进过的基于MCST的策略选择每步走子，使用对弈胜者作有价值的样本

​		下棋：一旦搜索完成，就会返回搜索概率$\pi \propto N^{1 / \tau}$其中N是来自根（root）的每次走子访问的次数，$\tau$是控制温度的参数；下出一步走子是通过采样搜索概率$\pi_t $完成的，然后转移到下一个状态$s_{t+1}$

​	end

​	给本场对弈评分以给出最终奖励$r_T \in \{-1,+1\}$

​	for 上一场比赛中的每个步骤t do

​		$z_t \leftarrow \pm r_T$，从当前棋手的角度得到的对弈胜者

​		将数据存储为$(s_t,\pi_t, z_t)$

​	end

​	在自我对局的前一/多次迭代的所有时间步骤上采样数据$(s,\pi,z)$

​	//训练神经网络权重$\theta_i$

​	//在单个步骤中间同时执行策略评估（通过$(z-v)^2$）和策略改进（通过$-\pi^Tlogp$)从而优化损失函数Loss

​	调整神经网络$(p,v)=f_{\theta_{i=1}}(s)$：以最小化预测的价值v和自我对弈胜者z之间的误差，并且最大化神经网络走子概率p以及搜索概率$\pi$

​	具体来说就是通过在损失函数上执行梯度下降来调整参数$\theta​$

​	$(p, v)=f_{\theta_{i}}(s)$与$ l=(z-v)^{2}-\pi^{T} \log p+c\left\|\theta_{i}\right\|^{2}$

end

```



### MCTS蒙特卡洛搜索树---走子演算Rollout

蒙特卡洛搜索树（Monte-Carlo Tree Search）是一种*大智若愚*的方法，它的基本思想是：

首先模拟一盘对决，使用的思路很简单，**随机**

- 面对一个空白棋盘 $\vec{s_0}$，最初我们对棋盘一无所知，假设所有落子的方法**分值**都相等，设为`1`
- 之后，**【随机】**从`361`种方法中选一种走法 $\vec{a_0}$，在这一步后，棋盘状态变为$\vec{s_1}$。之后假设对方也和自己一样，**【随机】**走了一步，此时棋盘状态变为$\vec{s_2}$
- 重复以上步骤直到 $\vec{s_n}$并且双方分出胜负，此时便完整的模拟完了一盘棋，我们假设一个变量`r`，胜利记为1，失败则为0

那么问题就来了，如果这一盘赢了，那意味着这一连串的下法至少比对面那个二逼要明智一些，毕竟我最后赢了，那么我把这次落子方法$ (\vec{s_0},\vec{a_0})$ 记下来，并把它的分值变化：

新分数=初始分数+r

同理，可以把之后所有随机出来的落子方法 $ (\vec{s_i},\vec{a_i})$都应用上式，即都加`1`分。之后开始第二次模拟，这一次，我们对棋盘不是一无所知了，至少在$\vec{s_0}$ 状态我们知道落子方法$\vec{a_0}$的分值是2，其他都是1，我们使用这个数据的方法是：在这次**随机**中，**我们随机到** $\vec{a_0}$**状态的概率要比其他方法高一点**。

之后，我们不断重复以上步骤，这样，那些看起来不错（以最后的胜负来作为判断依据）的落子方案的分数就会越来越高，并且这些落子方案也是比较有前途的，会被更多的选择。
$$
\operatorname{score}(\vec{s})=\left( \begin{array}{cccc}{r_{11}} & {r_{12}} & {\cdots} & {r_{1 n}} \\ {r_{21}} & {r_{22}} & {\cdots} & {r_{2 n}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {r_{n 1}} & {r_{n 2}} & {\cdots} & {r_{n n}}\end{array}\right)
$$

如上述公式所述，`n=19`，每一个状态$\vec{s}$ 都有一个对应的每个落子点的分数，只要模拟量足够多，那么可以覆盖到的$\vec{s}$状态就越多，漏洞就越来越小（可以思考李世石的神之一手，是否触及到了AlphaGo1.0的软肋呢？即没有考虑到的状态$\vec{s}$ ）

蒙特卡洛搜索树的方法十分的深刻精巧，充满的创造力，它有一些很有意思的特点：

- 没有任何人工决策的`if else`逻辑，完全依照规则本身，通过不断的想象（随机）来进行自我对弈，最后提升这一步的质量。有意思的是，其实这也是遵照了人类下棋的思维模式（**模仿**，只是这一次模仿的不是下棋风格，而是人类思考的方式。十分奇妙，人从飞鸟中受到启发发明了飞机，从鱼身上受到启发发明了潜艇，现在，机器学习的程序，通过学习人类使自身发生进化），人类中，水平越高的棋手，算的棋越多，只是人类对于每一个落子的判断能力更加强大，思考中的棋路，也比**随机**方式有效的多，但是机器胜在量大，暴力的覆盖到了很多情况。*注意，这一个特点也为之后的提高提供了思路*。
- MCTS可是持续运行。这种算法在对手思考对策的时候自己也可以思考对策。在对方思考落子的过程中，MCTS也可以继续进行演算，在对面落子后，在用现在棋盘的情况进行演算，并且之前计算的结果一定可以用在现在情况中，因为对手的下的这步棋，很可能也在之前演算的高分落子选择内。这一点十分像人类
- MCTS是**完全可并行**的算法

`Aja Huang`很快意识到这种方法的缺陷在哪里：初始策略（或者说**随机的落子方式**）太过简单。就如同上面第一条特点所说，人类对每种 $\vec{s}$ （棋型）都要更强的判断能力，那么我们是否可以用 $P_{human(\vec{s})}$ 来代替随机呢？

`Aja Huang`改进了MCTS，每一步不使用随机，而是现根据$P_{human(\vec{s})}$计算得到  $\vec{a}$ 可能的概率分布，以这儿概率为准来挑选下一个  $\vec{a}$。一次棋局下完之后，新分数按照下面的方式来更新

新分数=调整后的初始分+通过模拟的赢棋概率

如果某一步被随机到很多次，就应该主要依据模拟得到的概率而非 $P_{human(\vec{s})}$ ，就是说当盘数不断的增加，模拟得到的结果可能会好于 $P_{human(\vec{s})}$得到的结果。

所以 $P_{human(\vec{s})}​$ 的初始分会被打个折扣，这也是公式2-3中的调整后的初始分的由来

调整后的初始分=$P_{human(\vec{s})}​$/(被随机到的次数+1)

如此一来，就在整张地图上利用 $P_{human(\vec{s})}$快速定位了比较好的落子方案，也增加了其他位置的概率。实际操作中发现，此方案不可行，因为计算这个$P_{human(\vec{s})}$ **太慢了太慢了**

一次 $P_{human(\vec{s})}$的计算需要`3ms`，随机算法`1us`，慢了3000倍，所以，`Aja huang`训练了一个简化版本的 $P_{human_fast(\vec{s})}$ ，把神经网络层数、输入特征减少，耗时下降到`2us`，基本满足了要求。

更多的，策略是，先以 $P_{human(\vec{s})}$ 开局，走前面大概20步，之后再使用 $P_{human_fast(\vec{s})}$ 走完剩下的到最后。兼顾速度和准确性。

综合了深度卷积神经网络和MCTS两种方案，此时的围棋程序已经可以战胜所有其他电脑，虽然和其他人类职业选手还有一定的差距。

2015年2月，`Aja Huang`在Deepmind的同事在顶级学术期刊nature上发表的文章 [Human-level control through deep reinforcement learning](http://gnusha.org/~nmz787/pdf/Human-level_control_through_deep_reinforcement_learning.pdf) 用神经网络打游戏。这篇文章，给AlphaGo提供的了新的方向



## AlphaGo 的Value Work



### 强化学习---局面函数Value Work

强化学习（Reinforcement learning）用来实现**左右互搏和自我进化**

#### 利用强化学习增强棋力

参考这种思路，`Aja Huang`给围棋也设计了一个评价函数 $v(\vec{s})$ 。此函数的功能是：**量化评估围棋局面**。使用$v(\vec{s})$可以让我们在MCTS的过程中**不用走完全局**（走完全盘耗时耗力，效率不高）就发现**已经必败**。

在利用 $P_{human(\vec{s})}$走了开局的20步后，**如果有一个**$v(\vec{s}_i)$**（i为当前状态）可以直接判断是否能赢，得到最后的结果r**，不需要搜索到底，可以从效率（剪枝，优化算法时间复杂度）上进一步增加MCTS的威力。

很可惜的，现有的人类棋谱不足以得出这个评价函数。所以`Aja Huang`决定用**机器和机器对弈**的方法来创造新的对局，也就是AlphaGo的左右互搏

#### 自对弈

![](http://ww1.sinaimg.cn/large/006ocvumgy1g1vi8sih6ij32020qc1h1.jpg)

- 先用 $P_{human(\vec{s})}$和 $P_{human(\vec{s})}$对弈，比如1万盘，得到1万个新棋谱，加入到训练集中，训练出 $P_{human-1(\vec{s})}$。
- 使用$P_{human-1(\vec{s})}$和$P_{human-1(\vec{s})}$对弈，得到另1万个新棋谱，加入训练集，训练出$P_{human-2(\vec{s})}$。
- 同理，进行多次的类似训练，训练出$P_{human-n(\vec{s})}$，给最后的新策略命名为$P_{human-plus(\vec{s})}$

（感觉一下，这个$P_{human-plus(\vec{s})}$ 应该挺强力的！这里回顾一下$P_{human(\vec{s})}$是什么：是一个函数，$\vec{a}=f(\vec{s})$ 可以计算出当前 $\vec{s}$下的落子$\vec{a}$ 的分布概率）

使用$P_{human-plus(\vec{s})}$和$P_{human(\vec{s})}$进行对弈，发现$P_{human-plus(\vec{s})}$胜率80%，自对弈的方法被证明是有效的。（这里有一个想法，我在之前，一直加粗随机，之所以自对弈有效，就是因为整过MCTS过程中从来没有放弃过**随机**，如此一来，大量的计算，就更可能覆盖到更多的可能性，对提高棋力可以产生有效的作用同时。因为概率的问题，不断的自我对弈肯定造成下棋的路数集中，后面也会有体现）

但是事实并没有那么美好，`Aja Huang`发现，使用$P_{human-plus(\vec{s})}$来代替$P_{human(\vec{s})}$进行MCTS反而**棋力会下降**。

`Aja Huang`认为是$P_{human-plus(\vec{s})}$走棋的路数太集中，而MCTS需要更加发散的选择才能有更好的效果

#### 计算局部评价函数Value Network

考虑到$P_{human-plus(\vec{s})}$的下法太过集中，`Aja Huang`计算 $v(\vec{s})$的策略是：

- 开局先用$P_{human(\vec{s})}$走`L`步，有利于生成更多局面
- 即使如此，`Aja Huang`还是觉得局面不够多样，为了进一步扩大搜索空间，在`L+1`步时，完全随机一个 $\vec{a}$ 落子，记下这个状态 $v(\vec{s}_{L+1})$
- 之后使用$P_{human-plus(\vec{s})}$来进行对弈，直到结束时获得结果`r`，如此不断对弈，由于`L`也是一个随机数，我们可以得到，**开局、中盘、官子**等不同阶段的很多局面$\vec{s}$，和这些局面对应的结果`r`
- 有了这些训练样本$ ⟨\vec{s},r⟩$，还是使用**神经网络**，把最后一层改成**回归**而非**分类**（这里不是用的分类，而是用的**回归，拟合**），就得到了一个 **$v(\vec{s})$  来输出**赢棋的概率**