任务型对话中的对话策略学习DPL

对话系统按**功能**来划分的话，分为**闲聊型、任务型、知识问答型和推荐型**。在不同类型的聊天系统中，DM也不尽相同。

1. 闲聊型对话中的DM就是对上下文进行序列建模、对候选回复进行评分、排序和筛选等，以便于NLG阶段生成更好的回复；
2. 任务型对话中的DM就是在NLU(领域分类和意图识别、槽填充)的基础上，进行对话状态的追踪（DST）以及对话策略的学习（DPL），以便于DPL阶段策略的学习以及NLG阶段澄清需求、引导用户、询问、确认、对话结束语等。如果不太明白每个阶段的具体流程，可以看看我之前发的文章“[任务型对话系统公式建模&&实例说明](https://zhuanlan.zhihu.com/p/48268358)”。
3. 知识问答型对话中的DM就是在问句的类型识别与分类的基础上，进行文本的检索以及知识库的匹配，以便于NLG阶段生成用户想要的文本片段或知识库实体。
4. 推荐型对话系统中的DM就是进行用户兴趣的匹配以及推荐内容评分、排序和筛选等，以便于NLG阶段生成更好的给用户推荐的内容。

DPL中的状态建模和实例说明 DM=DST+DPL


![img](https://pic1.zhimg.com/80/v2-4618872015a407c968fb71f6f9b8c9f8_hd.jpg)

任务型对话的整体结构

![img](https://pic1.zhimg.com/80/v2-1a9acb0290b6bef15302acab1ace9594_hd.jpg)

DST在对话系统中的位置

![img](https://pic3.zhimg.com/80/v2-86717d0836eccdd4927732fcd2530302_hd.jpg)

DPL在对话系统中的位置

这里说一下dialogue act，act包括两类：用户的act和系统的act

针对用户时，主要是识别用户的act，dialogue act对应于SLU，包括意图和领域识别（一般是分类）+槽填充（一般是序列标注）

针对系统时，主要是识别系统act，dialogue act对应于DPL，表明在限制条件（之前的累积目标、对话历史等）下系统要执行的动作（接下来的策略），这个动作可能不是追求当前收益最大化，而是未来收益最大化。

至于dialogue act是个什么任务，**分类、序列标注、结构预测都OK**。**其对应的输入输出应该就是SLU的输入输出、DPL的输入输出。**

**DPL建模和实例说明**

说到DPL就离不开DST，所以这里再重新回顾一下DST的建模，然后再对DPL建模。


何谓对话状态？其实状态St是一种**包含0时刻到t时刻的对话历史、用户目标、意图和槽值对的数据结构**，这种数据结构可以供DPL阶段学习策略（比如定机票时，是询问出发地还是确定订单？）并完成NLG阶段的回复。

![img](https://pic2.zhimg.com/80/v2-258318fe7d1f65c5dae3d31c4ef9cec1_hd.jpg)

对话状态追踪DST：作用是根据领域(domain)/意图(intention)、槽值对(slot-value pairs)、之前的状态及之前的系统的action等来追踪当前状态。

它的**输入是Un（n时刻的意图和槽值对，也叫用户Action）、An-1（n-1时刻的系统Action）和Sn-1（n-1时刻的状态），输出是Sn（n时刻的状态）**。

S𝑛 = {Gn,Un,Hn}，Gn是用户目标、Un同上、Hn是聊天的历史，Hn= {U0, A0, U1, A1, ... , U𝑛−1, A𝑛−1}，S𝑛 =f(S𝑛−1,A𝑛−1,U𝑛)。
DST涉及到两方面内容：**状态表示、状态追踪**。另外为了解决领域数据不足的问题，DST还有很多迁移学习(Transfer Learning)方面的工作。比如基于特征的迁移学习、基于模型的迁移学习等。

DPL基于当前状态(state)决定系统需要采取action。它的输入是Sn，输出是An。

S𝑛 = {Gn,Un,Hn}，An是系统的Action，A𝑛 ={Dn, {Ai, Vi}}，Dn是对话类型，Ai、Vi是第i轮对话的attribute和其value。DPL一般会建模成**强化学习或深度强化学习**(从不同维度区分的话有非常非常多的变种，后面一一介绍)。

另外为了解决领域数据不足的问题，DPL还有很多迁移学习(Transfer Learning)方面的工作。比如线性模型迁移学习、高斯过程迁移学习、BCM迁移学习等。

### DPL的整体框架

前面也说了，DPL一般建模成强化学习或深度强化学习，下面看看DPL的框架。是很典型的强化学习在NLP领域的应用，因为DPL很适合强化学习（分步决策）。

![img](https://pic4.zhimg.com/80/v2-67539902b4d8f61b8986c3f3f1d18c0b_hd.jpg)

![img](https://pic4.zhimg.com/80/v2-5bc09e56e620003e6d338731fae7f0f3_hd.jpg)


![img](https://pic4.zhimg.com/80/v2-2153a653bbf90431fce9bd006798d8db_hd.jpg)

下面分别介绍一下对话系统中的不同DPL技术。
**3.1 Value Based DPL**([k-Nearest Neighbor Monte-Carlo Control Algorithm
for POMDP-based Dialogue Systems](<http://mi.eng.cam.ac.uk/~sjy/papers/lgjk09.pdf>),Gaši ́c et al. 2014）（Li et al., Interspeech 2009）（ Daubigney et al., 2012）**

**Lefevre et al., SIGDIAL 2009**
k近邻+ 蒙特卡罗算法+ POMDP(部分可观察马尔可夫决策过程)做DPL，这个题目起的很贴近原文，也是这方面开创性的工作之一，虽然时间有点久远，但还不失是一篇好文，**尤其是一些场景下的工程实现，因为本方法适合加规则和trick**。

![img](https://pic1.zhimg.com/80/v2-bfed542a4befc7a2b8b4caab429932c4_hd.jpg)

![img](https://pic1.zhimg.com/80/v2-80a0d99107e96c6cba15a3ab37f93b74_hd.jpg)

**Gaši ́c et al. 2014**

高斯过程 + POMDP(部分可观察马尔可夫决策过程)做DPL，本文一作是对话领域非常非常有名的学者，本文也算是开创性工作，不过感觉除了公式就是公式，看起来可能比较枯燥。另外，这个文章感觉对现在（深度学习和强化学习火起来之后）的论文没多大帮助，所以不再赘述。



**Li et al., Interspeech 2009**

LSPI-FFS（最小二乘策略迭代+快速特征选择）做DPL，具有很高的效率，并且可以从静态语料库或在线中学习。另外，这个方法能够更好地处理在对话模拟中评估的大量特征，设计者可以提出一大组潜在的有用特征，因此在当时达到了SOTA的效果。



**Daubigney et al., 2012**

POMDP(部分可观察马尔可夫决策过程)+离线学习做DPL。提出了一个样本有效、在线和非策略强化学习算法来学习最优策略。该算法结合到一个紧凑的非线性值函数表示（即多层感知器），能够处理**大规模系统**。之前在线学习的，一般处理的规模比较受限。



**3.2 Policy Based DPL（Jurcícek et al. 2011）（Wen et al. 2016b）（Su et al. 2016）**

**Jurcícek et al. 2011**

Natural Actor and Belief Critic: 基于POMDP利用强化学习算法来学习DPL的参数。本文的理论还是很扎实的，分析也比较到位。另外提出了几个算法，我感觉还是不错的。感兴趣的可以看看，如果不明白，咱们可以讨论。这几个算法我大概都看过了，也基本都懂。



**[Continuously learning neural dialogue management. Wen et al. 2016b](<https://arxiv.org/pdf/1606.02689.pdf>)**

**前面哪些论文比较久远，只有一部分参考价值，这篇文章非常有价值，算是端到端任务型对话的开创性工作之一，同时也是代表性工作之一，说他是做任务型对话必看的论文之一也不为过，质量非常高，本文几个作者在这个领域也都很高产。这篇论文，我后面应该会单独细讲，感兴趣的可以先看看，也可以等我分享。**

![img](https://pic1.zhimg.com/80/v2-1208051eed4cb6d627bd373558a141b8_hd.jpg)

![img](https://pic3.zhimg.com/80/v2-e3b557c77bec325bec28f04634303e46_hd.jpg)



**[A Network-based End-to-End Trainable Task-oriented Dialogue System,Su et al. 2016](<https://arxiv.org/pdf/1604.04562.pdf>)**

跟上一篇基本上是同一拨作者，同一时期做的类似的内容。本文主要体现出DM中的**持续学习。**系统首先通过监督学习从一组对话数据中学习，然后通过强化学习不断改进其行为，所有这些都基于梯度的单一模型上做。使用强化学习进一步提高了模型在两种交互设置下的性能，特别是在高噪声条件下。

![img](https://pic3.zhimg.com/80/v2-da864a2ffb8f287887eb16ace44d5716_hd.jpg)系统结构





**3.3 Actor Critic DPL（Su et al., SIGDIAL 2017）**

**Su et al., SIGDIAL 2017**

基于监督学习，利用Actor-Critic强化学习做DPL，具有很强的样本有效性。针对深度强化学习算法学习速度慢、初始性能差的问题，本文提出了两种兼容的方法。主要解决了**效率问题和早期阶段学习的问题**。

![img](https://pic1.zhimg.com/80/v2-a152f1fbbbd53127a57ce82af1054f1c_hd.jpg)



**3.4 基于迁移学习的DPL （Genevay and Laroche 2016** **）（Gaši ́c et al. 2015b** **）（Gaši ́c et al. 2015）**

**Genevay and Laroche 2016**

**本文主要针对**User Adaptation，提出了迁移强化学习方法，定义跨领域的相似度计算方法，仅将不同的数据点传输到目标域。



**Gaši ́c et al. 2015b**

基于高斯过程的Q-learning，传递均值函数和协方差函数依赖于跨域核函数，Cardinality based slot matching。



**Gaši ́c et al. 2015**

BCM（Bayesian Committee Machine）迁移学习的Q-learning，BCM是在不同领域的不同数据集上训练的高斯过程策略的集合，本文提出了基于熵的跨域核函数。





**3.5 其他方法**

除了以上方法，还有 Online Training DPL （[Su et al., Interspeech 2015](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.03386) ）（[Su et al., ACL 2016](https://link.zhihu.com/?target=https%3A//www.aclweb.org/anthology/P/P16/P16-1230.pdf)）；Interactive RL DPL （[Shah et al., 2016](https://link.zhihu.com/?target=https%3A//static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45734.pdf)）等。这些方法都有很不错的参考价值，但是他们不是只关注DPL一方面了，还会关注对话系统中的其他东西，所以不再单独讲解，感兴趣的可以私下找我一起讨论和学习。

![img](https://pic3.zhimg.com/80/v2-05675e5722f9cd1aeeee626274d65076_hd.jpg)

![img](https://pic4.zhimg.com/80/v2-d8864cd38d76f86c9cfd9d8ab0ad0783_hd.jpg)



**简单总结下，目前DPL相关的论文其实在任务型对话中相对算少的，至少远远少于SLU和DST，很多方法比较久远，而最新的一些方法又容易忽略DPL（毕竟DPL相比DST更容易点），更甚至很多DPL跟DST或NLG已经joint一起训练和学习了。目前主流的方法是（深度）强化学习。不过我感觉这一块还是值得深挖的。**

**4.不同DPL方法的对比**
**以上介绍了多种对话系统中的DPL技术，下面简单总结下它们的优势和劣势。**

![img](https://pic4.zhimg.com/80/v2-913f6f2a861e2b584598a582e947e5d7_hd.jpg)


**5.DPL技术的评估**
**任何一项技术想要取得进步，那么他的评测方法是至关重要的（就相当于目标函数之于机器学习算法），所以我列出一些关于DPL的评估。遗憾的是，目前DPL的评估我感觉并不成熟，这也是制约DPL发展的一个重要原因，如果谁能想出更好的评估方法或整理出一个业内公认的高质量数据集，那么一定会在DPL（甚至是对话系统）领域有一席之地，引用量也会蹭蹭的上涨。**

![img](https://pic2.zhimg.com/80/v2-e7f4915993cfe36f980c4a2cff098039_hd.jpg)

- CapsNet 内容 ：<https://www.jiqizhixin.com/articles/2017-11-05>


## 从DCGAN到SELF-MOD：GAN的模型架构发展一览

**好的生成器架构能加速GAN的收敛，或者提升GAN的效果**

## DCGAN

DCGAN 出自文章 ***Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks*** [2]。要说它做了什么事情，其实也简单：**它提出了一种生成器和判别器的架构，这个架构能极大地稳定 GAN 的训练，以至于它在相当长的一段时间内都成为了 GAN 的标准架构。** 

说起来简单，但事实上能做到这个事情很不容易，因为直观上“合理”的架构有很多，从各种组合中筛选出近乎最优的一种，显然是需要经过相当多的实验的。

而正因为 DCGAN 几乎奠定了 GAN 的标准架构，所以有了 DCGAN 之后，GAN 的研究者们可以把更多的精力放到更多样的任务之上，不再过多纠结于模型架构和稳定性上面，从而迎来了 GAN 的蓬勃发展。

**DCGAN所提出的架构大致如下：**

1. 生成器和判别器均不使用pooling层，而采用（带步长的）卷积层；其中判别器采用普通卷积Conv2D，而生成器采用反卷积Decov2D
2. 在生成器和判别器上均使用Batch Normalization
3. 在生成器除输出层外的所有层上使用ReLU激活函数，而输出层使用tanh激活函数
4. 在判别器的所有层上使用LeakyReLU激活函数
5. 卷积层之后不使用全连接层
6. 判别器的最后一个卷积层之后也不用Global Pooling，而是直接用Flatten

DCGAN 的结构示意图如下：



![img](https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglYbnFPqsBKVXLz5aXjJL3NshulAP411lAibyuJv1kercU9SUYdQRH6JyShZ1nCLJicw6jg8oqVfVNQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**▲** DCGAN的判别器架构(左）和生成器架构（右）

几个要点：



1. 卷积和反卷积的卷积核大小为 4*4 或者 5*5；



2. 卷积和反卷积的 stride 一般都取为 2；



3. 对于判别器来说，第一层卷积后一般不用 BN，而后面都是“Conv2D+BN+LeakReLU”的组合模式，直到 feature map 的大小为 4*4；



4. 对于生成器来说，第一层是全连接，然后 reshape 为 4*4 大小，然后是“Conv2D+BN+ReLU”的组合模式，最后一层卷积则不用 BN，改用 tanh 激活；相应地，输入图片都要通过除以 255 然后乘以 2 减去 1，来缩放到 -1～1 之间。



虽然从参数量看可能很大，但事实上 DCGAN 很快，而且占显存不算多，所以很受大家欢迎。因此虽然看起来很老，但至今仍然很多任务都在用它。至少在快速实验上，它是一种优秀的架构。

**DCGAN的问题**



公认的说法是，由于 DCGAN 的生成器中使用了反卷积，而反卷积固有地存在**“棋盘效应（Checkerboard Artifacts）”**，这个棋盘效应约束了DCGAN的生成能力上限。关于棋盘效应，详细可以参考 **Deconvolution and Checkerboard Artifacts** [3]（强烈推荐，超多效果图示）。



![img](https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglYbnFPqsBKVXLz5aXjJL3NgVQicFpOyWsaPbhPC2qJib9eiao3tD9nmF0uhCA3Z2lFdeCC9lBibfoqoQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**▲** 棋盘效应图示，体现为放大之后出现如国际象棋棋盘一样的交错效应。图片来自文章 *Deconvolution and Checkerboard Artifacts*



准确来说，棋盘效应不是反卷积的问题，而是 **stride > 1** 的固有毛病，这导致了卷积无法“各项同性”地覆盖整张图片，而出现了交错效应，如同国际象棋的棋盘一般。而反卷积通常都要搭配 stride > 1 使用，因此通常认为是反卷积的问题。



事实上，除了反卷积，**膨胀卷积也会有棋盘效应**，因为我们可以证明膨胀卷积在某种转化下，其实等价于 stride > 1 的普通卷积。



另一方面，笔者估计还有一个原因：**DCGAN 的非线性能力也许不足**。分析过 DCGAN 结果的读者会留意到，如果输入的图片大小固定后，整个 DCGAN 的架构基本都固定的，包括模型的层数。



唯一可以变化的似乎就只有卷积核大小（通道数也可以稍微调整，但其实调整空间不大），改变卷积核大小可以在一定程度上改变模型的非线性能力，但改变卷积核大小仅仅改变了模型的宽度，而对于深度学习来说深度可能比宽度更重要。问题就是对于 DCGAN 来说，没有一种自然而直接的方法来增加深度。

**ResNet模型**



由于以上原因，并且随着 ResNet 在分类问题的日益深入，自然也就会考虑到 ResNet 结构在 GAN 的应用。事实上，目前 GAN 上主流的生成器和判别器架构确实已经变成了 ResNet，基本结果图示如下：



![img](https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhglYbnFPqsBKVXLz5aXjJL3NovruKG3LXd66C25LhaLLI892bXiaQZuBhI4J60NxaOofiaeGgMzzBH4Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**▲** 基于ResNet的判别器架构(左）和生成器架构（右），中间是单个ResBlock的结构



可以看到，其实基于 ResNet 的 GAN 在整体结构上与 DCGAN 并没有太大差别（这进一步肯定了 DCGAN 的奠基作用），主要的特点在于：



1. 不管在判别器还是生成器，均去除了反卷积，只保留了普通卷积层；



2. 卷积核的大小通常统一使用 3*3 的，卷积之间构成残差块；



3. 通过 AvgPooling2D 和 UpSampling2D 来实现上/下采样，而 DCGAN 中则是通过 stride > 1 的卷积/反卷积实现的；其中 UpSampling2D 相当于将图像的长/宽放大若干倍；



​      4. 由于已经有残差，所以激活函数可以统一使用 ReLU，当然，也有一些模型依然使用 LeakyReLU，其实区别不大；



5. 通过增加 ResBlock 的卷积层数，可以同时增加网络的非线性能力和深度，这也是 ResNet 的灵活性所在；



6. 一般情况下残差的形式是 x+f(x)，其中 f 代表卷积层的组合；不过在 GAN 中，模型的初始化一般要比常规分类模型的初始化更小，因此稳定起见，有些模型干脆将其改为 x+α×f(x)，其中 α 是一个小于 1 的数，比如 0.1，这样能获得更好的稳定性；



7.  有些作者认为 BN 不适合 GAN，有时候会直接移除掉，或者用 LayerNorm 等代替。



**个人总结**



我没有认真考究过首先把 ResNet 用在 GAN 中是哪篇文章，只知道 PGGAN、SNGAN、SAGAN 等知名 GAN 都已经用上了 ResNet。ResNet 的 stride 都等于 1，因此足够均匀，不会产生棋盘效应。



然而，ResNet 并非没有缺点。虽然从参数量上看，相比 DCGAN，ResNet 并没有增加参数量，有些情况下甚至比 DCGAN 参数量更少，**但 ResNet 比 DCGAN 要慢得多，所需要的显存要多得多。**



这是因为 ResNet 层数更多、层之间的连接更多，所以导致梯度更复杂，并且并行性更弱了（同一层卷积可以并行，不同层卷积是串联的，无法直接并行），结果就是更慢了，更占显存了。



还有，棋盘效应实际上是一种非常细微的效应，也许仅仅是在高清图生成时才能感受到它的差异。



事实上在我的实验中，做 128*128 甚至 256*256 的人脸或 LSUN 生成，并**没有明显目测到 DCGAN 和 ResNet 在效果上的差异，但是 DCGAN 的速度比 ResNet 快 50% 以上**，在显存上，DCGAN 可以直接跑到 512*512 的生成（单个 1080ti），而 ResNet 的话，跑 256*256 都有些勉强。



因此，如果不是要 PK 目前的最优 FID 等指标，我都不会选择 ResNet 架构。