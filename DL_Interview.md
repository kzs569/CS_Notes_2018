
# DL Interview 深度学习面试题收集

- *题目来源*：https://github.com/ShanghaiTechAIClub/DLInterview


## 1. 数学相关(Math)

- 概率与数理统计

  - 一个袋子里有100个黑球和100个白球，每次随机拿出两个球丢掉，如果丢掉的是不同颜色的球，则从其他地方补充一个黑球到袋子里，如果颜色相同，则补充一个白球到袋子里。问：最后一个球是黑球和白球的概率分别为多大？

- 智商题
    -  A容器中有4L沙子，B容器中有4L米，假设米和沙子密度一样。有一个C容器是300ml, 第一步，用C从A中舀300ml到B中，混合均匀，第二步，用C从B冲舀300ml到A中混合均匀。再重复第一步和第二步，问这四步之后，A中的米和B中的沙子谁多？

## 2. 机器学习基础相关(ML)

### 2.1. 概率与统计相关
- **Relationship between Maximum likelihood and distance metrix?**
    * [solution]: 
-----------
- **如何理解最小二乘与最大似然估计**
    * （https://www.zhihu.com/question/20447622）
    * 论及本质，其实两者只是用不同的度量空间来进行的投影，OLS的度量是L2 norm distance，而极大似然的度量是Kullback-Leibler divergence.
    * 最大似然估计就是去找到那个（组）参数估计值，使得前面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。这时是求样本所有观测的联合概率最大化，是个连乘积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），得到最大似然估计值。
    * 实例理解 
        * 最小二乘 设想一个例子，教育程度和工资之间的关系。我们观察到的数据无非就是一个教育程度，对应着一个工资。我们希望的自然是找到两者之间的规律：如果把教育程度的初中、高中、大学、研究生及博士定义为1234的话，我们希望找到类似于工资=1000 +2000x教育程度  的这种规律，其中1000和2000是我们需要从数据里面发现的，前者称之为底薪，后者称之为教育增量薪水。如果我们就观察到两个数据，那解起来很简单，直接把两个数据带进去，二元一次方程组，就得到底薪和教育程度增量薪水之间的关系。这个在图上就体现为两点决定一条直线：
        ![](https://pic1.zhimg.com/50/v2-fae27da14b1efdeb88c87b1f40b0ff3d_hd.jpg)
        但是如果现在有三个数据，怎么办呢？如果这三个点不在一条线上，我们就需要作出取舍了，如果我们取任意两个点，那么就没有好好的利用第三个点带来的新信息，并且因为这三个点在数据中的地位相同，我们如何来断定应该选用哪两个点来作为我们的基准呢？这就都是问题了。这个时候我们最直观的想法就是『折衷』一下，在这三个数据，三条线中间取得某种平衡作为我们的最终结果，类似于图中的红线这样：
        ![](https://pic4.zhimg.com/50/v2-faae8e8d26791cc11245c91ee48eb5ce_hd.jpg)那怎么取平衡呢？那我们现在必须引入误差的存在，也就是我们要承认观测到的数据中有一些因素是不可知的，不能完全的被学历所解释。而这个不能解释的程度，自然就是每个点到红线在Y轴的距离。 但是我们尽管痛苦的承认了有不能解释的因素，但是我们依然想尽可能的让这种『不被解释』的程度最小，于是我们就想最小化这种不被解释的程度。因为点可能在线的上面或者下面，故而距离有正有负，取绝对值又太麻烦，于是我们就直接把每个距离都取一个平方变成正的，然后试图找出一个距离所有点的距离的平方最小的这条线，这就是最小二乘法了，简单粗暴而有效。
        * 而极大似然则更加的有哲理一些。还用上面的例子，我们观察到了三个点，于是我们开始反思，为什么我们观察到的是这三个点而不是另外三个？大千世界，芸芸众生，这么多人都有不同的工资，不同的学历，但是偏偏这三个点让我给观察到了。这肯定说明了某种世界的真相。</p><p><br></p><p>什么世界的真相呢？因为我们观察到了这三个点，反过来说，冥冥之中注定了这三个点被我们观察到的概率可能是最大的。所以我们希望找到一个特定的底薪和教育增量薪水的组合，让我们观察到这三个点的概率最大，这个找的过程就是极大似然估计。</p><p><br></p><p>具体的做法很简单，因为底薪和教育增量薪水虽然我们不知道，但是它一定存在，所以是个固定的值，能够随机变动的就是我们观察不到的神秘误差，那么给定一组底薪和教育增量薪水，必然存在一个唯一的误差与之对应，共同组合成了我们看到的数据。比如说，我们观察到一个人是：</p><p>高中毕业（学历变量=2） 工资 4500，如果我们假定工资=1000 +2000x教育程度的话，那么理论上工资应该是5000，而我们观察到了4500，所以这个时候误差为500。而误差=500，根据我们假设的误差的概率函数，总是存在一个概率与之相对应的（这个概率的分布我们可以假设）。而极大似然估计，就是把我们观察到每个样本所对应的误差的概率乘到一起，然后试图调整参数以最大化这个概率的乘积。</p><p><br></p><p>其背后的直觉是：假想有一个神秘的超自然力量，他全知全能，自然也知道真实的数据背后的规律。他在你抽样之前先做了一次复杂的计算，把无数个可能的抽样中，最可能出现的那个抽样展示给你。于是你根据这个抽样，逆流而上，倒推出来了数据背后的真实规律。</p><p><br></p><p>总结一句话，最小二乘法的核心是权衡，因为你要在很多条线中间选择，选择出距离所有的点之和最短的；而极大似然的核心是自恋，要相信自己是天选之子，自己看到的，就是冥冥之中最接近真相的。
## 2. 线性代数相关


- **A和B是两个矩阵，求 tr（A'B）,A'指A的转置。（写出来后，我没有检查AB是否size相同，他说应该考虑这种边界条件，并说你考虑了AB是否空矩阵这是比较好的一点）**
## 4. 经典ML模型
### 4.1 SVM模型
- **讲一讲SVM，知道多少说多少？为什么要用对偶问题求解?(今日头条)**
-----------

- **简单讲一下BP是如何计算的？**
-----------

- **SVM加核函数用过吗？讲一讲其中的差别？(今日头条)**
    * 训练样本多，维度大就可以用核函数；如果样本少，用核函数比较容易过拟合
-----------

- **SVM在训练的时候有没有遇到hard example？(今日头条)**
    * SVM对hard example的应用很重要，先用一部分训练集训练一个模型，然后用剩下的一部分训练集的一部分做测试，把出错的再送入重新训练，重复二三次，效果会比较好
-----------

- **SVM核函数的选择？多项式核和RBF核的关系？**
-----------

- **说LR和SVM损失函数。**
-----------
### 4.2 PCA
- **讲一下PCA．为什么是方差最大? 为什么有![](http://latex.codecogs.com/gif.latex?XX^')?**
-----------

- **推导PCA。具体问很多为什么。为什么是方差最大化？你这个是方差吗？**

### 4.3 XGBoost
- **XGBoost了解吗**
## Others
- **讲一下生成模型和判别模型有什么区别**

-----------

- **你觉得滴滴打车的拼车价是怎么计算出来的，详细描述。（路径规划，订单预测之类的）**

- **你觉得滴滴打车的溢价，1.1倍，1.2倍，这些数值是怎么计算出来的？（订单预测，当前司机数量），目的是什么？**

- **推导BP神经网络的反向求导。（可以用均方损失函数）**

## 1.3. 深度学习相关(DL)

### 1. CNN
- **CNN为什么可以在CV/NLP/Speech等领域都可以使用？**
    * 输入数据的局部相关性
    * 权值共享是因为输入数据的局部特征具有平移不变性，即在不同位置具有共性的局部特征。

- **什么样的数据适合Deep learning?**
    * 数据要有局部相关性，像表格类数据就不适合

- **CNN卷积**
    - 卷积的时间复杂度

- **BP**
    * 梯度消失,爆炸的原因
    * 梯度消失
    * 为什么梯度相反的方向是函数下降最快的方向？
        https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/grad/grad.html

- **1*1的卷积核的作用**


### 2. 过拟合(Overfitting)
- **何为共线性，跟过拟合有啥关联**
    * 多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。

- **共线性会造成冗余，导致过拟合。**
    * 解决方法：排除变量的相关性／加入权重正则。

- **Overfitting如何解决**
    * Data augmentation
    * regularization such as dropout, BN, weight decay
    * ensembling


### 3. Loss
- **交叉熵和相对熵（KL散度）？**
    * L1,L2 regularization的原理


- **为什么神经网络的损失函数是非凸的**


### 4. RNN
- **RNN有什么缺陷以及LSTM是如何怎么解决的？**

### 5. GAN 
- **介绍一下GAN,讲一讲生成模型和判别模型以及他们的关系**


### 6. 网络结构
- **resnet、inception，attention分别描述**

### 7. 网络训练
- **深度学习的待学习的参数量和训练样本数量之间的关系**
### Others
- **深度学习是万能的吗？什么地方不适用，如果给你一个任务，你如何选择用深度学习还是传统的如SVM？**
    * （任务、数据量、数据特点）

## 1.4. 计算机视觉相关(CV)

### Low-level
- **讲一讲HOG算子，以及如何求梯度?(今日头条)**

-----------

- **求SIFT算子的步骤和优化方法?**

## 1.5. 基础算法题相关(AL)
### 1.二叉树
- **二叉搜索树删除某个节点,先说思路后写代码(今日头条)**
-----------

- **二叉树之字形层次遍历，（正反正反）**
### 2.链表

- **一个链表，依次输出第一个，倒数第一个，第二个，倒数第二个(今日头条)**
-----------

- **原址:变换为奇数在前　偶数在后，相对顺序不变**
    - void 函数
-----------

- **链表求和**
-----------


### 3.查找

- **给一个float数，求算术平方根．(今日头条)**
    * 折半查找
-----------


### 4.哈希
- **要实现一个哈希表应该怎么做(根据要哈希的内容选择合适的哈希函数　和　冲突解决方案)**
-----------

### 5.排序
- **一个0-1矩阵，每一排都是0在前1在后，问哪一排的1最多？（二分法）问有没有更简单的方法（每一排记录当前的最左边的1的位置，下一排的时候直接忽略右边的）问时间复杂度，猜测是O(m+n)**
-----------

### 6.搜索
- **描述迪杰斯特拉算法**
-----------

- **描述dfs和bfs,分别怎么实现？（栈和队列）**
-----------

### 7. 动态规划
- **动态规划是什么, 动态规划和带记忆的递归有什么区别？（自顶而下和自底而上)**
-----------
- **0-1背包问题的动态规划递归式怎么写？**
-----------

- **三角数堆，只能往左下或者右下走，从堆顶到堆底和最小是多少。（dp）**
-----------
### Others
- **若干个长度不同的数组，求最小区间，让每个数组都有数字在这个区间内?**

----------

- **三种砖块，分别是1*1,1*2,1*3，拼成1*N, 有多少种拼法？时间复杂度多少？(今日头条)**
    * 斐波那契数列
-----------

- **描述KMP算法**
------------

- **写代码：两个有序数组的合并**

## 1.6. 算法语言相关(LA)

### 1. Python
- **一个python的dict，按照key-value存储，如何按照value排序**

- **static的类有什么用。**

- **如何实现一个只能实例化一次的类。**

- **模板类？**

- **纯虚函数和虚函数的区别**
### 2. C++

- **一个dict, 如果是C++怎么做？一个map,怎么按照value排序？**

- ** `int *p=6; free(p)`这段程序有没有错？运行会发生什么。**

## 1.7 凸优化


## 1. 一阶优化
- ### 常见的一阶优化算法(SGD, Adam等)

- ### 为什么负梯度的方向是函数下降最快的方向？

- ### 函数下降最快的方向是唯一的吗？


## 2. 二阶优化
- ### 为什么Hessian半正定，函数为凸函数？是充要条件吗？

- ### 牛顿法怎么求Hessian矩阵，了解拟牛顿法么?

- ### 牛顿法和拟牛顿法(BFGS 和 L-BFGS)

- ### 海森矩阵的逆(绕开求解海森矩阵)


## 3. 收敛性分析
- ### SGD一定收敛吗？为什么？

- ### 分析一阶优化和二阶优化的收敛速度