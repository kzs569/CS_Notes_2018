# 多智能体强化学习算法

## Berkely Ray RLlib

Blog : [An Open Source Tool for Scaling Multi-Agent Reinforcement Learning](<https://rise.cs.berkeley.edu/blog/scaling-multi-agent-rl-with-rllib/>) [Scaling Multi-Agent Reinforcement Learning](<https://bair.berkeley.edu/blog/2018/12/12/rllib/>)

Docs : [RLlib: Scalable Reinforcement Learning](<https://ray.readthedocs.io/en/latest/rllib.html>)

我们刚在Ray RLlib 0.6.0版本中推出对多智能体强化学习的通用支持，

这篇博客是对MARL的一个简短教程也包括了我们如何设计RLlib

我们的目标是使MARL从提升现有的单智能体算法到通过大规模定制算法来训练

**Why multi-agent reinforcement learning?**

将问题建模为MARL可以带来哪些好处？

- A more natural decomposition of the problem. 

- Potential for more scalable learning 

  1.将单个整体智能体的行动和观察分解为许多的简单智能体不仅降低了智能体输入输出的维度，而且更有效的增加了每一步环境产生的训练数据

  2.将动作和观察空间按智能体分解与时间抽象有相同的作用，这种方法已经在单智能体提升学习效率上取得了成功；一些分层强化学习的方法也可以被看作是多智能体系统

  3.好的分解方法同样可以导致学习在不同环境间更可转移的策略，与之相对的是单个超级智能体或许会对特定环境的过拟合

RLlib's distributed algorithms : A2C/A3C, PPO, IMPALA, DQN, DDPG, Ape-X

In the remainder of this blog post we discuss **the challenges of multi-agent RL**, show how to train multi-agent policies with these existing algorithms, and also how to implement specialized algorithms that can deal with the **non-stationarity** and **increased variance of multi-agent environments**.

Building software for a rapidly developing field such as RL is challenging, multi-agent especially so. This is in part due to **the breadth of techniques** used to deal with the core issues that arise in multi-agent learning.

- Environment non-stationarity 

  the fact that environemnt dynamics are changing from the perspective of a single agent violates the Markov assumptions required for the convergence of Q-learning algorithms such as DQN. A number of algorithms have been proposed that help address this issue, LOLA, RIAL, and Q-MIX. At a high level, these algorithms take into account the account the accouts of the other agents during RL training, usually by being partially centralized for training, but decentralized during execution.

  Similarly, policy-gradient algorithms like A3C and PPO may struggle in multi-agent settings, as the **credit assignment problem** becomes increasingly harder with more agents. 

  One class of approaches here is to model the effect of other agents on the environment with *centralized value functions (the “Q” boxes in Figure 5) function*, as done in [MA-DDPG](https://arxiv.org/abs/1706.02275). 
  

So far we’ve seen two different challenges and approaches for tackling multi-agent RL. That said, in many settings training multi-agent policies using single-agent RL algorithms can yield surprisingly strong results. For example, OpenAI Five has been successful using only a combination of large-scale PPO and a specialized network model. The only considerations for multi-agent are the annealing of a “team spirit” hyperparameter that influences the level of shared reward, and a shared “max-pool across Players” operation in the model to share observational information.

The Credit Assignment Problem concerns with how the success of an overall system can be attributed to the various contributions of a systems components

 In a Reinforcement Learning setup, it is already hard to attribute an
outcome to a particular action in history. With the extension to Multi-Agent settings, this problem increases in complexity mutlifold. It is extremely doubtful that
the signal presented by an outcome (e.g. a win or a loss) contains enough information to make this inference. The naive approach of equally dividing the outcome
reward to each of the agents seldom makes sense. Interested audience can find a
detailed discussion in [Sutton, 1984] and behavorial analysis with different reward
functions in [Balch, 1997].

Another phenomena which arises due to partial observability is called the **“LazyAgent Problem” **[Sunehag et al., 2017] which particularly can occur in Cooperative environments. Learning can fail when one of the agent becomes inactive because when one agent learns a useful policy, the second agent can be discouraged from exploration so as to not affect the first agent’s performance.


## QMIX算法

Papers : [QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning](<https://arxiv.org/pdf/1803.11485.pdf>)

### 1.算法简述

QMIX是一个多智能体强化学习算法，具有如下特点： 

1. 学习得到分布式策略。 
2. 本质是一个值函数逼近算法。
3. 由于对一个联合动作-状态只有一个总奖励值，而不是每个智能体得到一个自己的奖励值，因此只能用于合作环境，而不能用于竞争对抗环境。 
4. QMIX算法采用集中式学习，分布式执行应用的框架。通过集中式的信息学习，得到每个智能体的分布式策略。 
5. 训练时借用全局状态信息来提高算法效果。是后文提到的VDN方法的改进。 
6. 接上一条，QMIX设计一个神经网络来整合每个智能体的局部值函数而得到联合动作值函数，VDN是直接求和。 
7. 每个智能体的局部值函数只需要自己的局部观测，因此整个系统在执行时是一个分布式的，通过局部值函数，选出累积期望奖励最大的动作执行。 
8. 算法使联合动作值函数与每个局部值函数的单调性相同，因此对局部值函数取最大动作也就是使联合动作值函数最大。 
9. 算法针对的模型是一个分布式多智能体部分可观马尔可夫决策过程（Dec-POMDP）。

###  2.预备知识
#### 2.1 多智能体强化学习核心问题
在多智能体强化学习中一个关键的问题就是**如何学习联合动作值函数**，*因为该函数的参数会随着智能体数量的增多而成指数增长，如果动作值函数的输入空间过大，则很难拟合出一个合适函数来表示真实的联合动作值函数。*另一个问题就是学得了联合动作值函数后，如何通过联合值函数提取出一个优秀的分布式的策略。这其实是单智能体强化学习拓展到MARL的核心问题。

#### 2.2 Dec-POMDP

Dec-POMDP是将POMDP拓展到多智能体系统。每个智能体的局部观测信息 ![o_{i,t}](https://www.zhihu.com/equation?tex=o_%7Bi%2Ct%7D) ，动作 ![a_{i,t}](https://www.zhihu.com/equation?tex=a_%7Bi%2Ct%7D)，系统状态为 ![s_t](https://www.zhihu.com/equation?tex=s_t) 。其主要新定义了几个概念，简要介绍几个主要的。每个智能体的动作-观测历史可表示为 ![\tau_i=(a_{i,0},o_{i,1},\cdots,a_{i,t-1},o_{i,t})](https://www.zhihu.com/equation?tex=%5Ctau_i%3D%28a_%7Bi%2C0%7D%2Co_%7Bi%2C1%7D%2C%5Ccdots%2Ca_%7Bi%2Ct-1%7D%2Co_%7Bi%2Ct%7D%29) ，表示从初始状态开始，该智能体的时序动作-观测记录，联合动作-观测历史 ![\tau=(\tau_1,\cdots,\tau_n)](https://www.zhihu.com/equation?tex=%5Ctau%3D%28%5Ctau_1%2C%5Ccdots%2C%5Ctau_n%29) 表示从初始状态开始，所有智能体的时序动作-观测记录。则每个智能体的分布式策略为 ![\pi_i(\tau_i)](https://www.zhihu.com/equation?tex=%5Cpi_i%28%5Ctau_i%29) ，其值函数为 ![Q_i(\tau_i,a_i;\theta_i)](https://www.zhihu.com/equation?tex=Q_i%28%5Ctau_i%2Ca_i%3B%5Ctheta_i%29) 都是跟动作-观测历史 ![\tau_i](https://www.zhihu.com/equation?tex=%5Ctau_i) 有关，而不是跟状态有关了。

#### 2.3  IQL

IQL（independent Q-learning）就是非常暴力的给每个智能体执行一个Q-learning算法，因为共享环境，并且环境随着每个智能体策略、状态发生改变，对每个智能体来说，环境是动态不稳定的，因此这个算法也无法收敛，但是在部分应用中也具有较好的效果。

#### 2.4  VDN

Papers : [Value-Decomposition Networks For Cooperative Multi-Agent Learning](<https://arxiv.org/pdf/1706.05296.pdf>)

VDN（value decomposition networks）也是采用对每个智能体的值函数进行整合，得到一个联合动作值函数。令 ![\tau=(\tau_1,\cdots,\tau_n)](https://www.zhihu.com/equation?tex=%5Ctau%3D%28%5Ctau_1%2C%5Ccdots%2C%5Ctau_n%29) 表示联合动作-观测历史，其中 ![\tau_i=(a_{i,0},o_{i,1},\cdots,a_{i,t-1},o_{i,t})](https://www.zhihu.com/equation?tex=%5Ctau_i%3D%28a_%7Bi%2C0%7D%2Co_%7Bi%2C1%7D%2C%5Ccdots%2Ca_%7Bi%2Ct-1%7D%2Co_%7Bi%2Ct%7D%29) 为动作-观测历史， ![a=(a_1,\cdots,a_n)](https://www.zhihu.com/equation?tex=a%3D%28a_1%2C%5Ccdots%2Ca_n%29) 表示联合动作。 ![Q_{tot}](https://www.zhihu.com/equation?tex=Q_%7Btot%7D) 为联合动作值函数， ![Q_i(\tau_i,a_i;\theta_i)](https://www.zhihu.com/equation?tex=Q_i%28%5Ctau_i%2Ca_i%3B%5Ctheta_i%29) 为智能体i的局部动作值函数，局部值函数只依赖于每个智能体的局部观测。VDN采用的方法就是直接相加求和的方式

![Q_{tot}=\sum_{i=1}^{n}Q_i(\tau_i,a_i,;\theta_i)](https://www.zhihu.com/equation?tex=Q_%7Btot%7D%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7DQ_i%28%5Ctau_i%2Ca_i%2C%3B%5Ctheta_i%29)

虽然 ![Q_i(\tau_i,a_i;\theta_i)](https://www.zhihu.com/equation?tex=Q_i%28%5Ctau_i%2Ca_i%3B%5Ctheta_i%29) 不是用来估计累积期望回报的，但是这里依然叫它为值函数。分布式的策略可以通过对每个 ![Q_i(\tau_i,a_i;\theta_i)](https://www.zhihu.com/equation?tex=Q_i%28%5Ctau_i%2Ca_i%3B%5Ctheta_i%29) 取max得到。

#### 2.5  DRQN

DRQN是一个用来处理POMDP（部分可观马尔可夫决策过程）的一个算法，其采用LSTM替换DQN卷基层后的一个全连接层，来达到能够记忆历史状态的作用，因此可以在部分可观的情况下提高算法性能。具体讲解可以看[强化学习——DRQN分析详解](https://zhuanlan.zhihu.com/p/54898904)。由于QMIX解决的是多智能体的POMDP问题，因此每个智能体采用的是DRQN算法。

### 3. QMIX

上文“多智能体强化学习核心问题”提到的就是QMIX解决的最核心问题。其是在VDN上的一种拓展，由于VDN只是将每个智能体的局部动作值函数求和相加得到联合动作值函数，虽然满足联合值函数与局部值函数单调性相同的可以进行分布化策略的条件，但是其没有在学习时利用状态信息以及没有采用非线性方式对单智能体局部值函数进行整合，使得VDN算法还有很大的提升空间。

QMIX就是采用一个混合网络对单智能体局部值函数进行合并，并在训练学习过程中加入全局状态信息辅助，来提高算法性能。

为了能够沿用VDN的优势，利用集中式的学习，得到分布式的策略。主要是因为对联合动作值函数取 ![argmax](https://www.zhihu.com/equation?tex=argmax) 等价于对每个局部动作值函数取 ![argmax](https://www.zhihu.com/equation?tex=argmax) ，其单调性相同，如下所示

![{\rm argmax}_uQ_{tot}(\tau,u)=\left( \begin{aligned} {\rm argmax}_{u_1}&Q_1(\tau_1,u_1) \\ &\vdots\\ {\rm argmax}_{u_n}&Q_n(\tau_n,u_n) \\ \end{aligned} \right)\qquad (1)](https://www.zhihu.com/equation?tex=%7B%5Crm+argmax%7D_uQ_%7Btot%7D%28%5Ctau%2Cu%29%3D%5Cleft%28+%5Cbegin%7Baligned%7D+%7B%5Crm+argmax%7D_%7Bu_1%7D%26Q_1%28%5Ctau_1%2Cu_1%29+%5C%5C+%26%5Cvdots%5C%5C+%7B%5Crm+argmax%7D_%7Bu_n%7D%26Q_n%28%5Ctau_n%2Cu_n%29+%5C%5C+%5Cend%7Baligned%7D+%5Cright%29%5Cqquad+%281%29)

因此分布式策略就是贪心的通过局部 ![Q_i](https://www.zhihu.com/equation?tex=Q_i) 获取最优动作。QMIX将(1)转化为一种单调性约束，如下所示

![\frac{\partial Q_{tot}}{\partial Q_i}\ge 0, \forall i\in \{1,2,\cdots,n\}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Q_%7Btot%7D%7D%7B%5Cpartial+Q_i%7D%5Cge+0%2C+%5Cforall+i%5Cin+%5C%7B1%2C2%2C%5Ccdots%2Cn%5C%7D)

若满足以上单调性，则(1)成立，为了实现上述约束，QMIX采用混合网络（mixing network）来实现，其具体结构如下所示

![img](https://pic2.zhimg.com/80/v2-d03d9d93cb31a14a43ff5956528e5159_hd.jpg)



图(c)表示每个智能体采用一个[DRQN](https://zhuanlan.zhihu.com/p/54898904)来拟合自身的Q值函数的到 ![Q_i(\tau_i,a_i;\theta_i)](https://www.zhihu.com/equation?tex=Q_i%28%5Ctau_i%2Ca_i%3B%5Ctheta_i%29) ，DRQN循环输入当前的观测 ![o_{i,t}](https://www.zhihu.com/equation?tex=o_%7Bi%2Ct%7D) 以及上一时刻的动作 ![a_{i,t-1}](https://www.zhihu.com/equation?tex=a_%7Bi%2Ct-1%7D) 来得到Q值。

图(b)表示混合网络的结构。其输入为每个DRQN网络的输出。**为了满足上述的单调性约束，混合网络的所有权值都是非负数，对偏移量不做限制，这样就可以确保满足单调性约束。**

为了能够更多的利用到系统的状态信息 ![s_t](https://www.zhihu.com/equation?tex=s_t) ，采用一种超网络（hypernetwork），将状态 ![s_t](https://www.zhihu.com/equation?tex=s_t) 作为输入，输出为混合网络的权值及偏移量。为了保证权值的非负性，采用一个线性网络以及绝对值激活函数保证输出不为负数。对偏移量采用同样方式但没有非负性的约束，混合网络最后一层的偏移量通过两层网络以及ReLU激活函数得到非线性映射网络。由于状态信息 ![s_t](https://www.zhihu.com/equation?tex=s_t) 是通过超网络混合到 ![Q_{tot}](https://www.zhihu.com/equation?tex=Q_%7Btot%7D) 中的，而不是仅仅作为混合网络的输入项，这样带来的一个好处是，如果作为输入项则 ![s_t](https://www.zhihu.com/equation?tex=s_t) 的系数均为正，这样则无法充分利用状态信息来提高系统性能，相当于舍弃了一半的信息量。

QMIX最终的代价函数为

![L(\theta)=\sum_{i=1}^b[(y_i^{tot}-Q_{tot}(\tau,a,s;\theta))^2]](https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3D%5Csum_%7Bi%3D1%7D%5Eb%5B%28y_i%5E%7Btot%7D-Q_%7Btot%7D%28%5Ctau%2Ca%2Cs%3B%5Ctheta%29%29%5E2%5D)

更新用到了传统的[DQN](https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_37895339/article/details/84822668)的思想，其中b表示从经验记忆中采样的样本数量， ![y^{tot}=r+\gamma \max_{a'} \overline Q(\tau',a',s';\overline \theta)](https://www.zhihu.com/equation?tex=y%5E%7Btot%7D%3Dr%2B%5Cgamma+%5Cmax_%7Ba%27%7D+%5Coverline+Q%28%5Ctau%27%2Ca%27%2Cs%27%3B%5Coverline+%5Ctheta%29) ， ![\overline Q(\tau',a',s';\overline \theta)](https://www.zhihu.com/equation?tex=%5Coverline+Q%28%5Ctau%27%2Ca%27%2Cs%27%3B%5Coverline+%5Ctheta%29) 表示目标网络。

由于满足上文的单调性约束，对 ![Q_{tot}](https://www.zhihu.com/equation?tex=Q_%7Btot%7D) 进行 ![argmax](https://www.zhihu.com/equation?tex=argmax) 操作的计算量就不在是随智能体数量呈指数增长了，而是随智能体数量线性增长，极大的提高了算法效率。

### 4. 小示例

原文中给了一个小示例来说明QMIX与VND的效果差异，虽然QMIX也不能完全拟合出真实的联合动作值函数，但是相较于VDN已经有了很大的提高。

如下图为一个两步合作矩阵博弈的价值矩阵

![img](https://pic1.zhimg.com/80/v2-0782a8562fbdb13278703090bd69f714_hd.jpg)



在第一阶段，只有智能体 ![1 ](https://www.zhihu.com/equation?tex=1+) 的动作能决定第二阶段的状态。在第一阶段，如果智能体 ![1](https://www.zhihu.com/equation?tex=1) 采用动作 ![A](https://www.zhihu.com/equation?tex=A) 则跳转到上图 ![{State 2A}](https://www.zhihu.com/equation?tex=%7BState+2A%7D) 状态，如果智能体 ![1](https://www.zhihu.com/equation?tex=1) 采用动作 ![B](https://www.zhihu.com/equation?tex=B) 则跳转到上图 ![State 2B](https://www.zhihu.com/equation?tex=State+2B) 状态，第二阶段的每个状态的价值矩阵如上两图所示。

现在分别用VDN与QMIX学习上述矩阵博弈各个状态的值函数矩阵，得到结果如下图所示

![img](https://pic2.zhimg.com/80/v2-92a2f7f0fbd1192bdd2d629f0f0da905_hd.jpg)



(a)为VDN拟合结果，(b)为QMIX拟合结果。可以从上图，VDN的结果是智能体 ![1](https://www.zhihu.com/equation?tex=1) 在第一阶段采用动作 ![A](https://www.zhihu.com/equation?tex=A) ，显然这不是最佳状态，而QMIX是智能体 ![1](https://www.zhihu.com/equation?tex=1) 在第一阶段采用动作 ![B](https://www.zhihu.com/equation?tex=B) ，得到了最大的累积期望奖励。由上可得QMIX的逼近能力比VDN更强，QMIX算法的效果更好。

## MADDPG

Papers : [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](<https://arxiv.org/pdf/1706.02275.pdf>)

Github : https://github.com/openai/maddpg

## 一、引言

本章介绍OpenAI 2017发表在NIPS 上的一篇文章，《Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments》。主要是将AC算法进行了一系列改进，使其能够适用于传统RL算法无法处理的复杂多智能体场景。

传统RL算法面临的一个主要问题是由于每个智能体都是在不断学习改进其策略，因此从每一个智能体的角度看，环境是一个动态不稳定的，这不符合传统RL收敛条件。并且在一定程度上，无法通过仅仅改变智能体自身的策略来适应动态不稳定的环境。由于环境的不稳定，将无法直接使用之前的经验回放等DQN的关键技巧。policy gradient算法会由于智能体数量的变多使得本就有的方差大的问题加剧。

MADDPG算法具有以下三点特征： 1. 通过学习得到的最优策略，在应用时只利用局部信息就能给出最优动作。 2. 不需要知道环境的动力学模型以及特殊的通信需求。 3. 该算法不仅能用于合作环境，也能用于竞争环境。

MADDPG算法具有以下三点技巧：

1. 集中式训练，分布式执行：训练时采用集中式学习训练critic与actor，使用时actor只用知道局部信息就能运行。critic需要其他智能体的策略信息，本文给了一种估计其他智能体策略的方法，能够只用知道其他智能体的观测与动作。
2. 改进了经验回放记录的数据。为了能够适用于动态环境，每一条信息由 ![(x,x', a_q,\cdots,a_n,r_1,\cdots,r_n)](https://www.zhihu.com/equation?tex=%28x%2Cx%27%2C+a_q%2C%5Ccdots%2Ca_n%2Cr_1%2C%5Ccdots%2Cr_n%29) 组成， ![x=(o_1,\cdots,o_n)](https://www.zhihu.com/equation?tex=x%3D%28o_1%2C%5Ccdots%2Co_n%29) 表示每个智能体的观测。
3. 利用策略集合效果优化（policy ensemble）：对每个智能体学习多个策略，改进时利用所有策略的整体效果进行优化。以提高算法的稳定性以及鲁棒性。

其实MADDPG本质上还是一个DPG算法，针对每个智能体训练一个需要全局信息的Critic以及一个需要局部信息的Actor，并且允许每个智能体有自己的奖励函数（reward function），因此可以用于合作任务或对抗任务。并且由于脱胎于DPG算法，因此动作空间可以是连续的。

## 二、背景知识

## 1. DQN

DQN的思想就是设计一个 ![Q(s,a|\theta)](https://www.zhihu.com/equation?tex=Q%28s%2Ca%7C%5Ctheta%29) 不断逼近真实的 ![Q(s,a)](https://www.zhihu.com/equation?tex=Q%28s%2Ca%29) 函数。其中主要用到了两个技巧：1. 经验回放。2. 目标网络。该技巧主要用来打破数据之间联系，因为神经网络对数据的假设是独立同分布，而MDP过程的数据前后有关联。打破数据的联系可以更好地拟合 ![Q(s,a)](https://www.zhihu.com/equation?tex=Q%28s%2Ca%29) 函数。其代价函数为：

$$L(\theta) = E_{s,a,r,s'}[(Q(s,a|\theta)-y)^2],\qquad \rm{where}\ y=r+\gamma max_{a'}\overline Q(s',a'|\overline \theta)$$

其中 ![\overline Q(s',a'|\overline \theta)](https://www.zhihu.com/equation?tex=%5Coverline+Q%28s%27%2Ca%27%7C%5Coverline+%5Ctheta%29) 表示目标网络，其参数更新与 ![\theta](https://www.zhihu.com/equation?tex=%5Ctheta) 不同步（滞后）。具体可以参看[值函数强化学习-DQN、DDQN和Dueling DQN算法公式推导分析](https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_37895339/article/details/84822668)。

## 2. SPG（stochastic policy gradient）

SPG算法不采用拟合Q函数的方式，而是直接优化累积回报来获得使回报最大的策略。假定参数化的策略为 ![\pi_\theta(a|s)](https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta%28a%7Cs%29) ，累积回报为 ![J(\theta)=E_{s\sim \rho^{\pi},a\sim \pi_\theta}[\sum_{t=0}^{\infty}\gamma^t r_t]](https://www.zhihu.com/equation?tex=J%28%5Ctheta%29%3DE_%7Bs%5Csim+%5Crho%5E%7B%5Cpi%7D%2Ca%5Csim+%5Cpi_%5Ctheta%7D%5B%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%5Cgamma%5Et+r_t%5D) 。为了使 ![J(\theta)](https://www.zhihu.com/equation?tex=J%28%5Ctheta%29) 最大化，直接对策略参数求导得到策略更新梯度：

![\nabla_{\theta} J(\theta)=E_{s\sim \rho^{\pi},a\sim \pi_\theta}[\nabla_{\theta}\log\pi_\theta(a|s)Q^\pi(s,a)]](https://www.zhihu.com/equation?tex=%5Cnabla_%7B%5Ctheta%7D+J%28%5Ctheta%29%3DE_%7Bs%5Csim+%5Crho%5E%7B%5Cpi%7D%2Ca%5Csim+%5Cpi_%5Ctheta%7D%5B%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%5Ctheta%28a%7Cs%29Q%5E%5Cpi%28s%2Ca%29%5D)

AC算法也可以由此推出，如果按照DQN的方法拟合一个 ![Q(s,a|\theta)](https://www.zhihu.com/equation?tex=Q%28s%2Ca%7C%5Ctheta%29) 函数，则这个参数化的 ![Q(s,a|\theta)](https://www.zhihu.com/equation?tex=Q%28s%2Ca%7C%5Ctheta%29) 函数被称为Critic， ![\pi_\theta(a|s)](https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta%28a%7Cs%29) 被称为Actor。具体可以参看，[随机策略梯度算法（stochastic-policy-gradient）](https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_37895339/article/details/84792752)。

## 3. DPG

上述两种算法都是针对随机策略， ![\pi_\theta(a|s)](https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta%28a%7Cs%29) 是一个在状态s对于各个动作a的条件概率分布。DPG针对确定性策略， ![\mu_\theta(s):S\to A](https://www.zhihu.com/equation?tex=%5Cmu_%5Ctheta%28s%29%3AS%5Cto+A) 是一个状态空间到动作空间的映射。其思想与SPG相同，得到策略梯度公式为

![\nabla_{\theta} J(\theta)=E_{s\sim \beta}[\nabla_{\theta}\mu_\theta(s)\nabla_a Q^\mu(s,a)|_{a=\mu_\theta(s)}]](https://www.zhihu.com/equation?tex=%5Cnabla_%7B%5Ctheta%7D+J%28%5Ctheta%29%3DE_%7Bs%5Csim+%5Cbeta%7D%5B%5Cnabla_%7B%5Ctheta%7D%5Cmu_%5Ctheta%28s%29%5Cnabla_a+Q%5E%5Cmu%28s%2Ca%29%7C_%7Ba%3D%5Cmu_%5Ctheta%28s%29%7D%5D)

DPG可以是使用AC的方法来估计一个Q函数，DDPG就是借用了DQN经验回放与目标网络的技巧，具体可以参看，[确定性策略强化学习-DPG&DDPG算法推导及分析](https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_37895339/article/details/84881872)。

## 三、MADDPG

## 1. 多智能体AC设计

MADDPG集中式的学习，分布式的应用。因此我们允许使用一些额外的信息（全局信息）进行学习，只要在应用的时候使用局部信息进行决策就行。这点就是Q-learning的一个不足之处，Q-learning在学习与应用时必须采用相同的信息。所以这里MADDPG对传统的AC算法进行了一个改进，Critic扩展为可以利用其他智能体的策略进行学习，这点的进一步改进就是每个智能体对其他智能体的策略进行一个函数逼近。

我们用 ![\theta=[\theta_1,\cdots,\theta_n]](https://www.zhihu.com/equation?tex=%5Ctheta%3D%5B%5Ctheta_1%2C%5Ccdots%2C%5Ctheta_n%5D) 表示n个智能体策略的参数， ![\pi=[\pi_1,\cdot,\pi_n]](https://www.zhihu.com/equation?tex=%5Cpi%3D%5B%5Cpi_1%2C%5Ccdot%2C%5Cpi_n%5D) 表示n个智能体的策略。针对第i个智能体的累积期望奖励 ![J(\theta_i)=E_{s\sim \rho^{\pi},a_i\sim \pi_{\theta_i}}[\sum_{t=0}^{\infty}\gamma^t r_{i,t}]](https://www.zhihu.com/equation?tex=J%28%5Ctheta_i%29%3DE_%7Bs%5Csim+%5Crho%5E%7B%5Cpi%7D%2Ca_i%5Csim+%5Cpi_%7B%5Ctheta_i%7D%7D%5B%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%5Cgamma%5Et+r_%7Bi%2Ct%7D%5D) ，针对随机策略，求策略梯度为：

![\nabla_{\theta_i}J(\theta_i)=E_{s\sim \rho^\pi,a_i\sim \pi_i}[\nabla_{\theta_i}\log\pi_i(a_i|o_i)Q_i^{\pi}(x,a_1,\cdots,a_n)]](https://www.zhihu.com/equation?tex=%5Cnabla_%7B%5Ctheta_i%7DJ%28%5Ctheta_i%29%3DE_%7Bs%5Csim+%5Crho%5E%5Cpi%2Ca_i%5Csim+%5Cpi_i%7D%5B%5Cnabla_%7B%5Ctheta_i%7D%5Clog%5Cpi_i%28a_i%7Co_i%29Q_i%5E%7B%5Cpi%7D%28x%2Ca_1%2C%5Ccdots%2Ca_n%29%5D)

其中 ![o_i](https://www.zhihu.com/equation?tex=o_i) 表示第i个智能体的观测， ![x=[o_1,\cdots,o_n]](https://www.zhihu.com/equation?tex=x%3D%5Bo_1%2C%5Ccdots%2Co_n%5D) 表示观测向量，即状态。 ![Q_i^{\pi}(x,a_1,\cdots,a_n)](https://www.zhihu.com/equation?tex=Q_i%5E%7B%5Cpi%7D%28x%2Ca_1%2C%5Ccdots%2Ca_n%29) 表示第i个智能体集中式的状态-动作函数。由于是每个智能体独立学习自己的 ![Q_i^\pi](https://www.zhihu.com/equation?tex=Q_i%5E%5Cpi) 函数，因此每个智能体可以有不同的奖励函数（reward function），因此可以完成合作或竞争任务。

上述为随机策略梯度算法，下面我们拓展到确定性策略 ![\mu_{\theta_i}](https://www.zhihu.com/equation?tex=%5Cmu_%7B%5Ctheta_i%7D) ，梯度公式为

![\nabla_{\theta_i}J(\mu_i)=E_{x,a\sim D}[\nabla_{\theta_i}\mu_i(a_i|o_i)\nabla_{a_i}Q_i^\mu(x,a_1,\cdots,a_n)|_{a_i=\mu_i(o_i)}]](https://www.zhihu.com/equation?tex=%5Cnabla_%7B%5Ctheta_i%7DJ%28%5Cmu_i%29%3DE_%7Bx%2Ca%5Csim+D%7D%5B%5Cnabla_%7B%5Ctheta_i%7D%5Cmu_i%28a_i%7Co_i%29%5Cnabla_%7Ba_i%7DQ_i%5E%5Cmu%28x%2Ca_1%2C%5Ccdots%2Ca_n%29%7C_%7Ba_i%3D%5Cmu_i%28o_i%29%7D%5D)

由以上两个梯度公式可以看出该算法与SPG与DPG十分类似，就像是将单体直接扩展到多体。但其实 ![Q_i^\mu](https://www.zhihu.com/equation?tex=Q_i%5E%5Cmu) 是一个非常厉害的技巧，针对每个智能体建立值函数，极大的解决了传统RL算法在Multi-agent领域的不足。D是一个经验存储（experience replay buffer），元素组成为 ![(x,x',a_1,\cdots,a_n,r_1,\cdots,r_n)](https://www.zhihu.com/equation?tex=%28x%2Cx%27%2Ca_1%2C%5Ccdots%2Ca_n%2Cr_1%2C%5Ccdots%2Cr_n%29) 。集中式的critic的更新方法借鉴了DQN中TD与目标网络思想

$$L(\theta_i)=E_{x,a,r,x'}[(Q_i^\mu(x,a_1,\cdots,a_n)-y)^2],\qquad \rm{where}\ y=r_i+\gamma \overline Q_i^{\mu'}(x',a_1',\cdots,a_n')|_{a_j'=\mu_j'(o_j)}\qquad $$(1)

![\overline Q_i^{\mu'}](https://www.zhihu.com/equation?tex=%5Coverline+Q_i%5E%7B%5Cmu%27%7D) 表示目标网络， ![\mu'=[\mu_1',\cdots,\mu_n']](https://www.zhihu.com/equation?tex=%5Cmu%27%3D%5B%5Cmu_1%27%2C%5Ccdots%2C%5Cmu_n%27%5D) 为目标策略具有滞后更新的参数 ![\theta_j'](https://www.zhihu.com/equation?tex=%5Ctheta_j%27) 。其他智能体的策略可以采用拟合逼近的方式得到，而不需要通信交互。

如上可以看出critic借用了全局信息学习，actor只是用了局部观测信息。MADDPG的一个启发就是，如果我们知道所有的智能体的动作，那么环境就是稳定的，就算策略在不断更新环境也是恒定的，因为模型动力学使稳定的 ![P(s'|s,a_1,\cdots,a_n,\pi_1,\cdots,\pi_n)=P(s'|s,a_1,\cdots,a_n)=P(s'|s,a_1,\cdots,a_n,\pi_1',\cdots,\pi_n')](https://www.zhihu.com/equation?tex=P%28s%27%7Cs%2Ca_1%2C%5Ccdots%2Ca_n%2C%5Cpi_1%2C%5Ccdots%2C%5Cpi_n%29%3DP%28s%27%7Cs%2Ca_1%2C%5Ccdots%2Ca_n%29%3DP%28s%27%7Cs%2Ca_1%2C%5Ccdots%2Ca_n%2C%5Cpi_1%27%2C%5Ccdots%2C%5Cpi_n%27%29)。

## 2. 估计其他智能体策略

在(1)式中，我们用到了其他智能体的策略，这需要不断的通信来获取，但是也可以放宽这个条件，通过对其他智能体的策略进行估计来实现。每个智能体维护n-1个策略逼近函数 ![\hat \mu_{\phi_i^j}](https://www.zhihu.com/equation?tex=%5Chat+%5Cmu_%7B%5Cphi_i%5Ej%7D) 表示第i个智能体对第j个智能体策略 ![\mu_j](https://www.zhihu.com/equation?tex=%5Cmu_j) 的函数逼近。其逼近代价为对数代价函数，并且加上策略的熵，其代价函数可以写为

![L(\phi_i^j)=-E_{o_j,a_j}[\log \hat \mu_{\phi_i^j}(a_j|o_j)+\lambda H(\hat \mu_{ \phi_i^j})]](https://www.zhihu.com/equation?tex=L%28%5Cphi_i%5Ej%29%3D-E_%7Bo_j%2Ca_j%7D%5B%5Clog+%5Chat+%5Cmu_%7B%5Cphi_i%5Ej%7D%28a_j%7Co_j%29%2B%5Clambda+H%28%5Chat+%5Cmu_%7B+%5Cphi_i%5Ej%7D%29%5D)

只要最小化上述代价函数，就能得到其他智能体策略的逼近。因此可以替换(1)式中的y。

$$y=r_i+\gamma \overline Q_i^{\mu'}(x',\hat \mu_{\phi_i^j}'^1(o_1),\cdots,\hat \mu_{\phi_i^j}'^n(o_n))$$

在更新 ![Q_i^\mu](https://www.zhihu.com/equation?tex=Q_i%5E%5Cmu) 之前，利用经验回放的一个采样batch更新 ![\hat \mu_{\phi_i^j}](https://www.zhihu.com/equation?tex=%5Chat+%5Cmu_%7B%5Cphi_i%5Ej%7D) 。

## 3. 策略集合优化（policies ensemble）

这个技巧也是本文的一个亮点。多智能体强化学习一个顽固的问题是由于每个智能体的策略都在更新迭代导致环境针对一个特定的智能体是动态不稳定的。这种情况在竞争任务下尤其严重，经常会出现一个智能体针对其竞争对手过拟合出一个强策略。但是这个强策略是非常脆弱的，也是我们希望得到的，因为随着竞争对手策略的更新改变，这个强策略很难去适应新的对手策略。

为了能更好的应对上述情况，MADDPG提出了一种策略集合的思想，第i个智能体的策略 ![\mu_i](https://www.zhihu.com/equation?tex=%5Cmu_i) 由一个具有K个子策略的集合构成，在每一个训练episode中只是用一个子策略 ![\mu_{\theta^{(k)}_i}](https://www.zhihu.com/equation?tex=%5Cmu_%7B%5Ctheta%5E%7B%28k%29%7D_i%7D) （简写为 ![\mu_i^{(k)}](https://www.zhihu.com/equation?tex=%5Cmu_i%5E%7B%28k%29%7D)）。对每一个智能体，我们最大化其策略集合的整体奖励 ![J_e(\mu_i)=E_{k\sim {\rm unif(1,K)},s\sim \rho^\mu,a\sim\mu_i^{(k)}}[\sum_{t=0}^\infty \gamma^t r_{i,t}]](https://www.zhihu.com/equation?tex=J_e%28%5Cmu_i%29%3DE_%7Bk%5Csim+%7B%5Crm+unif%281%2CK%29%7D%2Cs%5Csim+%5Crho%5E%5Cmu%2Ca%5Csim%5Cmu_i%5E%7B%28k%29%7D%7D%5B%5Csum_%7Bt%3D0%7D%5E%5Cinfty+%5Cgamma%5Et+r_%7Bi%2Ct%7D%5D) 。并且我们为每一个子策略k构建一个记忆存储 ![D_i^{(k)}](https://www.zhihu.com/equation?tex=D_i%5E%7B%28k%29%7D) 。我们优化策略集合的整体效果，因此针对每一个子策略的更新梯度为

![\nabla_{\theta_i^{(k)}}J_e(\mu_i)=\frac{1}{K}E_{x,a\sim D_i^{(k)}}[\nabla_{\theta_i^{(k)}}\mu_i^{(k)}(a_i|o_i)\nabla_{a_i}Q^{\mu_i}(x,a_1,\cdots,a_n)|_{a_i=\mu_i^{(k)}(o_i)}]](https://www.zhihu.com/equation?tex=%5Cnabla_%7B%5Ctheta_i%5E%7B%28k%29%7D%7DJ_e%28%5Cmu_i%29%3D%5Cfrac%7B1%7D%7BK%7DE_%7Bx%2Ca%5Csim+D_i%5E%7B%28k%29%7D%7D%5B%5Cnabla_%7B%5Ctheta_i%5E%7B%28k%29%7D%7D%5Cmu_i%5E%7B%28k%29%7D%28a_i%7Co_i%29%5Cnabla_%7Ba_i%7DQ%5E%7B%5Cmu_i%7D%28x%2Ca_1%2C%5Ccdots%2Ca_n%29%7C_%7Ba_i%3D%5Cmu_i%5E%7B%28k%29%7D%28o_i%29%7D%5D)

以上就是MADDPG所有的内容，仿真效果也很好的证明了MADDPG在多智能体系统中的有效性。 [MADDPG](https://link.zhihu.com/?target=https%3A//github.com/openai/maddpg)是OpenAI给的仿真代码。

## 对手建模 Opponent Modeling

在多智能体学习领域，有一种历史悠久的著名方法，叫做虚构对策（fictitious play），DeepMind提都没有提；多智能体环境中的对手建模，有一个完整的推理框架，叫做交互式的部分可观察马尔科夫决策过程（Interactive POMDPs），DeepMind也完全忽略了。

[Opponent Modeling in Deep Reinforcement Learning](https://arxiv.org/pdf/1609.05559.pdf)

[Modeling Others using Oneself in Multi-Agent Reinforcement Learning](<https://arxiv.org/pdf/1802.09640.pdf>) NYU,FAIR  Using opponent goal as addtional input

[Learning Policy Representations in Multiagent Systems](<https://arxiv.org/pdf/1806.06464.pdf>) Stanford,CMU,OpenAI Using policy representation to cluser, classify and RL(using opponent's embedding as addtional input)

**Review :**

- Abstract : We propose a general learning framework for modeling agent behavior in any multiagent system **using only a handful of interaction data. **Our framework **casts agent modeling as a representation learning problem**. Consequently, we construct a novel objective inspired by **imitation learning** and **agent identification** and design an algorithm for **unsupervised learning of representations of agent policies**. 

- Environments : RoboSumo, ParticleWorld

- Learning framework(GAN, IL,)

Formally, we are interested in learning a policy $\pi_{\phi}^{(i)} : \mathcal{S} \times \mathcal{A} \rightarrow[0,1]$ for an agent i given access to observation and action pairs from interaction episode(s) involving the agent. 

For behavioral cloning, we maxmize the following(negative) cross-entrogy objective:
$$
\mathbb{E}_{e \sim E_{i}}\left[\sum_{\langle o, a\rangle \sim e}\left[\log \pi_{\phi}^{(i)}(a | o)\right]\right]
$$
where the expectation is over interaction episodes of agent i and the optimization is over the parameters $\phi$



![](http://ww1.sinaimg.cn/large/006ocvumgy1g2bj5vapn1j30k50j3gps.jpg)

Learning individual policies for every agent  can be computationally and statistically prohibitive for large-scale multiagent systems and precludes generalization across the behaviors of such agents.

Learning a single policy for all agents increases sample efficiency but comes at the cost of reduced modeling flexibility in simulating diverse agent behaviors.

To offset this dichotomy, learning **a single conditional policy network.**

Loss of conditional policy network:
$$
\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}_{e_{1} \sim E_{i},e_2\sim E_i/e_1}\left[\sum_{\langle o, a\rangle \sim e_{1}} \log \pi_{\phi, \theta}\left(a | o, e_{2}\right)\right]
$$


## Communication

[Emergence of Grounded Compositional Language in Multi-Agent Populations](https://arxiv.org/pdf/1703.04908.pdf)     

[Learning to Communicate with Deep Multi-Agent Reinforcement Learning](http://papers.nips.cc/paper/6398-learning-multiagent-communication-with-backpropagation.pdf) Communicate discrete action

[Learning Multiagent Communication with Backpropagation](http://papers.nips.cc/paper/6398-learning-multiagent-communication-with-backpropagation.pdf)  Communicate hidden state



## Others

[Cooperative Multi-Agent Control Using Deep Reinforcement Learning](<http://ala2017.it.nuigalway.ie/papers/ALA2017_Gupta.pdf>) Github : <https://github.com/sisl/MADRL>

[Relational Inductive Bias for Deep Reinforcement Learning](https://openreview.net/pdf?id=HkxaFoC9KQ)

The goal of this paper is to enhance model-free deep reinforcement techniques with relational knowledge about the environment such that the agents can learn interpretable state representations which subsequently improves sample complexity and generalization ability of the approach. The relational knowledge works as an inductive bias for the reinforcement learning algorithm and provides better understanding of complex environment to the agents.

To achieve this, the authors focus on distributed advantage actor-critic algorithm and propose a shared relational network architecture for parameterizing the actor and critic network. The relational network contains a self-attention mechanism inspired from recent work in that area. Using these new modules, the authors conduct evaluation experiments on  two different environment - synthetic Box World and real-world StarCraft-II minigames where they analyze the performance against non-relational counterparts, visualize the attention weights for interpretability and test on out-of-training tasks for generalizability.
